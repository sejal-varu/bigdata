
sd = [('mehul', 99,93),('Jane',93,100),('jill',100,80)]
df1 = spark.createDataFrame(sd)
df1.show()
df1 = spark.createDataFrame(sd,schema=['name','phy','chem'])
df1.show()
df1.columns
df1.dtypes
df1.printSchema()
df2 = df1.select(df1.phy,df1.chem)
df2
df2.show()
df1.show()
df1.phy
from pyspark.Sql.types import *
from pyspark.sql.types import *
sch = StructType('name', StringType(), false),
sch = StructType(['name', StringType(), false),
sch = StructType([
'name', StringType(), false,
'phy', FloatType(),False,
Struct
sch = StructType([
StructField( 'name', StringType(), false),
StructField( 'phy', FloatType(),False),
StructField( 'chem', FloatType(),False)
])
sch = StructType([
StructField( 'name', StringType(), False),
StructField( 'phy', FloatType(),False),
StructField( 'chem', FloatType(),False)
])
sch
df3 = spark.createDataFrame(sd,schema=sch)
df3 = spark.createDataFrame(sd,schema= sch)
sd
sd = [('mehul', 99.00, 93.00), ('Jane', 93.00, 100.00), ('jill', 100.00, 80.00)]
df3 = spark.createDataFrame(sd,schema= sch)
df3.show()
df4 = df3.select(df3.name, df3.phy, df3.chem, (df3.phy + df3.chem).alias('Total') )
df4.show()
df5 = df3.withColumn((df3.phy + df3.chem).alias('Total'))
df5 = df3.withColumn(df3.phy + df3.chem)
df5 = df3.withColumn('total', df3.phy + df3.chem)
d5=f5.show()
d5=df5.show()
df6 = df5.where(df5.phy >= 90)
df6.show()
df6 = df5.where(df5.phy >= 99)
df6.show()
df7 = df5.select(df5.phy, df5.chem).where(df5.phy >=99)
df7.show()
df1.show()
df1.agg({'phy': 'sum'}).show()
import 
from pyspark.sql import functions as F
df1.agg(F.max(df1.phy).alias('MaxPhy'), F.min(df1.chem).alias('MinChem').alias('MinChem')).show()
df1.agg(F.mean(df1.phy).alias('MeanPhy'), F.mean(df1.chem).alias('MeanChem')).show()
df1.describe()
df1.describe().shw()
df1.describe().show()
df1select(df1.phy,df1.chem).describe().show()
df1.select(df1.phy,df1.chem).describe().show()
df1.count()
import readlines
import Readlines
import Readlines;
import Readline;
import readlines
import readline
readline.write_history_file('/home/sejal/Desktop/Datascience/spark_sql')
