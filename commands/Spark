gedit ~/.bashrc

export SPARK_HOME=/home/sejal/Documents/datascience/spark-2.3.0-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin
export PYSPARK_PYTHON=python3



On terminal:

pyspark

opens spark shell :

RDD for marks

>>> r1 = sc.parallelize([5,6,3,4,1,10,10,9,6,7,9])

Deduct 1 mark from all students marks:

>>> r2 = r1.map(lambda ele : ele - 1 )

Print:
>>> r1.collect()
[5, 6, 3, 4, 1, 10, 10, 9, 6, 7, 9]

>>> r2.collect()
[4, 5, 2, 3, 0, 9, 9, 8, 5, 6, 8]


Count:
>>> r1.count()
11

>>> lines = ['cat mat rat sat','cat mat sunday monday','python mat sat scala java']
>>> r5 = sc.parallelize(lines)
>>> r6 = r5.map(lambda line : line.split(' '))
>>> r7 = r5.flatMap(lambda line : line.split(' '))

r1 = sc.parallelize([1,3,5,10,4])
>>> r2 = sc.parallelize([3,5,5,8])
>>> r3 = r1.union(r2)
>>> r3.collect()

>>> r4 = r1.intersection(r2)
>>> r5 = r1.subtract(r2)
>>> r4 = r3.distinct()
>>> r1.mean()

>>> r4.max()
10
>>> r4.min()

>>> cbv = r2.countByValue()
>>> cbv
defaultdict(<class 'int'>, {8: 1, 3: 1, 5: 2})






>>> i1 = sc.parallelize(['beer','sprite','milk','soda'])
>>> l2 = sc.parallelize(['maggie','nuggets','pampers'])
>>> i3 = i1.cartesian(i2)

>>> records = [(1900, 34), (1900, 30), (1901, 45), (1901, 23), (1901, 32), (1901,21)]
>>> recRdd = sc.parallelize(records)
>>> recRdd.collect()
[(1900, 34), (1900, 30), (1901, 45), (1901, 23), (1901, 32), (1901, 21)]

>>> years = recRdd.keys()
>>> years
PythonRDD[51] at RDD at PythonRDD.scala:48
>>> years.collect()
[1900, 1900, 1901, 1901, 1901, 1901]

>>> temps = recRdd.values()
>>> temps.collect()
[34, 30, 45, 23, 32, 21]

>>> recRdd.collect()
[(1900, 34), (1900, 30), (1901, 45), (1901, 23), (1901, 32), (1901, 21)]
>>> deductedTemps = recRdd.map(lambda ele: (ele[0], ele[1]-1) )

>>> deductedTemps.collect()
[(1900, 33), (1900, 29), (1901, 44), (1901, 22), (1901, 31), (1901, 20)]

>>> recRdd.mapValues(lambda temp : temp - 1).collect()
[(1900, 33), (1900, 29), (1901, 44), (1901, 22), (1901, 31), (1901, 20)]

>>> deductedTemps.countByKey()
defaultdict(<class 'int'>, {1900: 2, 1901: 4})

>>> deductedTemps.countByValue()
defaultdict(<class 'int'>, {(1901, 22): 1, (1901, 44): 1, (1900, 33): 1, (1900, 29): 1, (1901, 20): 1, (1901, 31): 1})


Get distinct temp and count
Method-1
>>> deductedTemps.values().countByValue()
defaultdict(<class 'int'>, {33: 1, 20: 1, 22: 1, 44: 1, 29: 1, 31: 1})

Method-2
>>> deductedTemps.map(lambda entry: (entry[1], entry[0])).countByKey()
defaultdict(<class 'int'>, {33: 1, 20: 1, 22: 1, 44: 1, 29: 1, 31: 1})


import readline
>>> readline.write_history_file("/home/sejal/Desktop/Datascience/spark_inclass")
>>> r1 = sc.textFile('/home/sejal/Documents/datascience/dataset/data/weather_data/weather_data_set_1900')
>>> r1.collect()
['3300|1900|0101|0300|23', '3300|1900|0101|0400|24', '3300|1900|0301|0200|26', '3300|1900|0305|0230|24', '3300|1900|0312|0100|30', '3300|1900|0412|0300|29', '3301|1900|0312|0100|34', '3301|1900|0412|0400|23']
>>> r1 = sc.textFile('/home/sejal/Documents/datascience/dataset/data/weather_data/')
>>> r1.collect()
['3300|1900|0101|0300|23', '3300|1900|0101|0400|24', '3300|1900|0301|0200|26', '3300|1900|0305|0230|24', '3300|1900|0312|0100|30', '3300|1900|0412|0300|29', '3301|1900|0312|0100|34', '3301|1900|0412|0400|23', '3300|1901|0101|0400|24', '3300|1901|0101|0500|45', '3300|1901|0301|0300|40', '3300|1901|0312|0200|34', '3300|1901|0412|0100|NA', '3301|1901|0312|0130|22', '3301|1901|0412|01500|21', '3302|1901|0102|0400|20', '3302|1901|0103|0500|24', '3302|1901|0203|0300|35', '3302|1901|0312|0200|20', '3302|1901|0412|0100|19', '3302|1901|0102|0400|26', '3302|1901|0104|0430|']
>>> r1.count()
22
>>> r2 = r1.map(lambda entry : (entry.split('|')[1], entry.split("|")[4])
... )
>>> r2.count()
22
>>> r2.collect()
[('1900', '23'), ('1900', '24'), ('1900', '26'), ('1900', '24'), ('1900', '30'), ('1900', '29'), ('1900', '34'), ('1900', '23'), ('1901', '24'), ('1901', '45'), ('1901', '40'), ('1901', '34'), ('1901', 'NA'), ('1901', '22'), ('1901', '21'), ('1901', '20'), ('1901', '24'), ('1901', '35'), ('1901', '20'), ('1901', '19'), ('1901', '26'), ('1901', '')]























